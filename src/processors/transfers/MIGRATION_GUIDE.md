# Transfer Processors Migration Guide

This document outlines the consolidation of transfer processors and recommended migration path.

## ğŸ“Š Analysis Summary

### TimescaleDB Usage Assessment: âœ… EXCELLENT FIT

**TimescaleDB is perfect for your transfers data** because:

1. **Time-series nature**: Token transfers are naturally time-series with timestamps
2. **High volume**: Transfer events are high-frequency data benefiting from compression
3. **Optimized aggregations**: 5-minute raw â†’ hourly aggregated with rolling averages
4. **Automatic policies**: Compression (1hr/1day) + retention (7d raw, 90d hourly)  
5. **Query performance**: Rolling window calculations (24h/7d/30d) are much faster

## ğŸ—ï¸ Previous Architecture Issues

The transfers folder had **significant redundancy**:

```
src/processors/transfers/
â”œâ”€â”€ token_transfer_timeseries.py      # TimescaleDB core (well-designed) 
â”œâ”€â”€ latest_transfers_processor.py     # Standalone script with mixed logic
â”œâ”€â”€ transfer_processors.py           # BaseProcessor + Parquet + Redis
â”œâ”€â”€ mev_transfer_processor.py        # MEV analysis using PostgreSQL
â”œâ”€â”€ legacy_jared_analysis.py         # Old analysis script
â””â”€â”€ tests/                           # Scattered test coverage
```

**Problems identified:**
- Duplicate logic between processors
- Mixed storage approaches (TimescaleDB + PostgreSQL + Redis)
- No clear "primary" processor
- Inconsistent patterns vs BaseProcessor architecture

## âœ¨ New Unified Architecture

```
src/processors/transfers/
â”œâ”€â”€ unified_transfer_processor.py    # ğŸ¯ PRIMARY: TimescaleDB + Redis + BaseProcessor
â”œâ”€â”€ mev_transfer_processor.py       # MEV analysis (keeps PostgreSQL for chain-specific tables)
â”œâ”€â”€ token_transfer_timeseries.py    # Low-level TimescaleDB operations
â”œâ”€â”€ transfer_processors.py          # ğŸ“› LEGACY: Use UnifiedTransferProcessor instead
â”œâ”€â”€ latest_transfers_processor.py   # ğŸ“› LEGACY: Use UnifiedTransferProcessor instead  
â”œâ”€â”€ legacy_jared_analysis.py        # ğŸ“› LEGACY: Kept for historical reference
â””â”€â”€ tests/                          # Comprehensive test coverage
    â”œâ”€â”€ test_unified_transfer_processor.py
    â””â”€â”€ test_mev_transfer_processor.py
```

## ğŸš€ UnifiedTransferProcessor Features

The new `UnifiedTransferProcessor` combines the best of all approaches:

### âœ… Storage Strategy
- **Raw Data**: 5-minute intervals â†’ TimescaleDB hypertables
- **Aggregated Data**: Hourly rollups with 24h/7d/30d averages  
- **Caching**: Redis for fast API access
- **Compression**: Automatic TimescaleDB compression policies
- **Retention**: Auto-cleanup (7d raw, 90d hourly)

### âœ… Processing Pipeline  
```python
Parquet Files â†’ Raw 5min Data â†’ Hourly Aggregation â†’ Redis Cache â†’ API
```

### âœ… BaseProcessor Integration
- Follows established `BaseProcessor` pattern
- Compatible with existing orchestration
- Proper error handling and logging
- Comprehensive test coverage (13 tests, all passing)

## ğŸ”„ Migration Path

### Phase 1: Immediate (Use UnifiedTransferProcessor)
```python
# OLD: Multiple processors with mixed patterns
from src.processors.transfers import LatestTransfersProcessor

# NEW: Single unified processor
from src.processors.transfers import UnifiedTransferProcessor

processor = UnifiedTransferProcessor("ethereum")
result = await processor.process(
    hours_back=24,
    min_transfers=100,
    store_raw=True,        # Store in TimescaleDB
    update_cache=True      # Update Redis cache  
)
```

### Phase 2: Gradual (Replace legacy usage)
1. **Update orchestrators** to use `UnifiedTransferProcessor`
2. **Migrate existing scripts** from `latest_transfers_processor.py` 
3. **Update APIs** to use unified caching methods
4. **Remove legacy files** after validation

### Phase 3: Cleanup (Remove redundant files)
```bash
# After successful migration, remove:
# src/processors/transfers/transfer_processors.py
# src/processors/transfers/latest_transfers_processor.py  
```

## ğŸ“‹ Usage Examples

### Basic Processing
```python
from src.processors.transfers import UnifiedTransferProcessor

processor = UnifiedTransferProcessor("ethereum")

# Process recent transfers
result = await processor.process(hours_back=24, min_transfers=100)
print(f"Processed {len(result.data)} top tokens")
```

### Advanced Usage
```python
# Custom processing with specific parameters
result = await processor.process(
    data_dir="/path/to/transfers", 
    hours_back=6,
    min_transfers=50,
    store_raw=True,     # Store 5-minute raw data
    update_cache=True   # Update Redis cache
)

# Get cached results (fast API responses)
top_tokens = await processor.get_cached_top_tokens(period='24h')
token_stats = await processor.get_token_stats("0xTokenAddress")
```

### Integration with MEV Analysis
```python
# MEV analysis remains separate (uses PostgreSQL for chain-specific tables)
from src.processors.transfers import MEVTransferProcessor

mev_processor = MEVTransferProcessor("ethereum")
mev_result = await mev_processor.process()
active_tokens = await mev_processor.get_most_active_tokens(hours_back=1)
```

## ğŸ§ª Testing

All processors have comprehensive test coverage:

```bash
# Run unified processor tests (13 tests)
uv run pytest src/processors/transfers/tests/test_unified_transfer_processor.py -v

# Run MEV processor tests (9 tests)  
uv run pytest src/processors/transfers/tests/test_mev_transfer_processor.py -v

# Run all transfer processor tests
uv run pytest src/processors/transfers/tests/ -v
```

## âš ï¸ Important Notes

### Keep TimescaleDB
- **DO NOT** migrate away from TimescaleDB
- It's the optimal solution for time-series transfer data
- Compression and retention policies save significant storage
- Query performance for rolling averages is superior

### Database Size Management âœ…
Your current retention policies are **excellent** and conservative:

```sql
-- Raw transfers (5-minute intervals)
Retention: 7 days
Compression: After 1 hour
Chunks: 5-minute intervals

-- Hourly aggregates  
Retention: 90 days
Compression: After 1 day
Chunks: 1-hour intervals
```

**Size estimates for Ethereum mainnet:**
- Raw data: ~50-100MB per day â†’ ~700MB max (7 days)
- Hourly data: ~5-10MB per day â†’ ~900MB max (90 days)
- **Total: < 2GB even with high activity**

**TimescaleDB benefits:**
- 10-100x compression ratios
- Automatic cleanup (no manual intervention)
- Query performance remains fast even with retention

### MEV Analysis Unified
- **MEVTransferProcessor eliminated** - queries MEV addresses directly from raw data
- More efficient: single data source, no duplicate storage
- Same functionality via `processor.get_mev_active_tokens()`

### Migration Strategy
- **Gradual migration** to avoid disruption
- **Legacy processors kept** during transition period
- **Comprehensive testing** before removing old code
- **Documentation updates** for API consumers

## ğŸ¯ Benefits Achieved

1. **Simplified Architecture**: Single primary processor vs multiple overlapping ones
2. **Better Performance**: TimescaleDB optimization + Redis caching  
3. **Consistent Patterns**: Follows BaseProcessor architecture
4. **Comprehensive Testing**: Full test coverage with mocking
5. **Clear Separation**: Transfer processing vs MEV analysis
6. **Future-Proof**: Extensible for additional transfer analysis needs

The unified approach maintains all existing functionality while providing a cleaner, more maintainable foundation for future development.